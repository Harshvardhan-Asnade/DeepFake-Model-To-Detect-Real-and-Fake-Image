# Deepfake Detection Using EfficientNetB0 and Fine-Tuning Strategies
**Author:** Harshvardhan Asnade  
**Date:** December 9, 2025

---

## Abstract

Deepfakes—synthetic media generated by Artificial Intelligence—have become a major threat to digital trust. This paper presents a high-performance deep learning model designed to classify images as either "Real" or "Fake" with high accuracy. Using the **EfficientNetB0** architecture, we implemented a two-phase training strategy: first using Transfer Learning to learn general features, and then applying Fine-Tuning to detect subtle artifacts. The final model achieved a **91.56% accuracy** on the validation dataset, demonstrating that advanced architecture coupled with careful training strategies can effectively detect AI-generated imagery on consumer hardware (Apple M4).

---

## 1. Introduction

### 1.1 The Problem
With the rise of Generative AI tools (like Midjourney, Stable Diffusion, and GANs), it is now easy to create hyper-realistic fake images. These "deepfakes" can be used for misinformation, identity theft, and fraud. Distinguishing them from real photos is becoming impossible for the human eye.

### 1.2 Our Solution
We proposed an automated solution using Computer Vision. Instead of building a brain from scratch, we used a powerful pre-existing AI brain (EfficientNet) and taught it to specifically look for the "glitches" and "fingerprints" left behind by AI generators (such as unnatural textures, lighting inconsistencies, or pixel artifacts).

---

## 2. Methodology (The Architecture)

We chose **EfficientNetB0** as our backbone. This is a modern Convolutional Neural Network (CNN) developed by Google, known for being both highly accurate and fast.

### 2.1 Model Structure
The model consists of two main parts:

1.  **The Feature Extractor (Base)**:
    *   **Architecture**: EfficientNetB0 (pre-trained on the massive ImageNet dataset).
    *   **Role**: It breaks down the image into mathematical patterns (edges, textures, shapes).
    *   **Input Shape**: 150x150 pixels (RGB).
    *   **Preprocessing**: Images are efficiently scaled using EfficientNet's internal standard (0-255 range normalization).

2.  **The Classifier (Head)**:
    *   We added a custom "Head" on top of the base to make the final decision.
    *   **Global Average Pooling**: Converts the complex feature map into a simple vector.
    *   **Dense Layer (512 Neurons)**: A large layer to process the features (activated by ReLU).
    *   **Batch Normalization**: Stabilizes the learning process.
    *   **Dropout (0.5)**: Randomly turns off 50% of neurons during training to prevent "memorizing" (overfitting).
    *   **Output Layer**: A single neuron with a **Sigmoid** activation function, outputting a score between 0 (Fake) and 1 (Real).

---

## 3. Training Strategy

To achieve >90% accuracy, we used a **Two-Phase Training Approach**.

### Phase 1: Transfer Learning (The "Warm Up")
*   **Goal**: Adapt the new "Head" to the dataset without disturbing the pre-trained knowledge of EfficientNet.
*   **Action**: We **froze** the EfficientNet base (made it un-trainable). We only trained the custom Head.
*   **Settings**: 
    *   Optimizer: Adam (Learning Rate: 0.001)
    *   Epochs: 10
*   **Result**: The model reached **~78% accuracy**. It learned broad differences but missed subtle details.

### Phase 2: Fine-Tuning (The "Deep Dive")
*   **Goal**: Squeeze out maximum performance by letting the AI adjust its deeper grasp of the image.
*   **Action**: We **unfroze the last 30 layers** of the EfficientNet base.
    *   *Crucial Detail*: We kept the BatchNormalization layers frozen to prevent destroying the model's internal statistics.
*   **Settings**:
    *   Optimizer: Adam (Learning Rate: **0.0001**) - *10x slower rate to be careful.*
    *   Epochs: 10
*   **Result**: The model rapidly improved from 78% to **91.56% accuracy**.

---

## 4. Experimental Setup

### 4.1 Hardware
*   **Device**: Apple MacBook Pro (M4 Chip).
*   **Acceleration**: Training was accelerated using the **Metal (MPS)** plugin for TensorFlow, allowing high-speed GPU processing on Mac.

### 4.2 Dataset & Preparation
*   **Structure**: The data was split into Train, Validation, and Test sets.
*   **Augmentation**: To update the model's robustness, training images were randomly:
    *   Rotated (up to 20 degrees)
    *   Shifted (width/height)
    *   Zoomed
    *   Horizontal Flipping
*   This prevents the model from relying on position or specific angles.

---

## 5. Results and Analysis

### 5.1 Final Metrics
After completing Phase 2, the model achieved:
*   **Validation Accuracy**: **91.56%**
*   **Validation Loss**: **0.2160**

### 5.2 Performance Curve
*   **Phase 1**: Accuracy plateaued around 78%.
*   **Phase 2**: Accuracy immediately jumped to 80%, then steadily climbed to 91.5% as the fine-tuning allowed the model to detect micro-artifacts.

### 5.3 Interpretation
The model is highly aggressive at detecting Fakes. The use of EfficientNetB0 proved superior to the earlier MobileNetV2 attempts (which stalled at ~76%). The fine-tuning step was the critical factor in breaking the 90% barrier.

---

## 6. Discussion: Comparative Analysis & Evolution

Reaching 91.56% accuracy was an iterative process involving multiple architectures and debugging phases. Below is the detailed evolution of the model.

### 5.1 Experiment A: MobileNetV2 (The Baseline)
Our initial approach used **MobileNetV2**, a lightweight model designed for mobile devices.
*   **Configuration**: MobileNetV2 Base + Custom Head (512 Dense).
*   **Result**: The model achieved reasonable performance, peaking at **76-79% accuracy**.
*   **Limitation**: While fast, MobileNetV2 lacked the capacity to detect the subtle "high-frequency" artifacts often present in deepfakes (e.g., texture mismatches in hair or iris patterns). It plateaued and could not break the 80% barrier despite extended training.

### 5.2 The "Random Guessing" Failure (Debugging Phase)
During the transition to EfficientNet, we encountered a critical failure where the model stalled at **50% accuracy** (random chance).
*   **Diagnosis**: A mismatch in **Data Preprocessing**.
    *   Our data loader was manually scaling pixels to `[0, 1]` (dividing by 255).
    *   EfficientNet expects raw inputs `[0, 255]` and handles scaling internally via its own preprocessing module.
*   **Impact**: This mismatch destroyed the image signal, causing the model to learn nothing (effectively blindly guessing).
*   **Fix**: We rewrote the preprocessing pipeline to use `tf.keras.applications.efficientnet.preprocess_input`, effectively solving the issue and allowing the model to learn immediately.

### 5.3 Experiment B: EfficientNetB0 (Base Training)
After fixing the input pipeline, we trained EfficientNetB0 with the base layers frozen (Transfer Learning).
*   **Result**: Accuracy reached **78%**.
*   **Observation**: This matched the best performance of MobileNetV2 but did so with fewer epochs, indicating a stronger potential learner.

### 5.4 Experiment C: EfficientNetB0 (Fine-Tuned - The Final Model)
To unlock the full potential, we applied the Fine-Tuning strategy described in Section 3.
*   **Action**: Unfrozen top 30 layers.
*   **Result**: Accuracy surged from 78% $\rightarrow$ **91.56%**.
*   **Conclusion**: The deep features learned by EfficientNet on ImageNet were good, but fine-tuning allowed it to specifically adapt those features to the domain of "Synthetic Media Detection."

| Model Architecture | Training Mode | Accuracy | Status |
| :--- | :--- | :--- | :--- |
| MobileNetV2 | Transfer Learning | 76.0% | Limited |
| EfficientNetB0 (Bugged) | Mismatched Inputs | 50.1% | Failed |
| EfficientNetB0 | Frozen Base | 78.4% | Good Baseline |
| **EfficientNetB0** | **Fine-Tuned** | **91.56%** | **SOTA Performance** |

---

## 7. Conclusion and Future Work

This project successfully developed a Deepfake Detection system capable of **91.56% accuracy**. By leveraging Transfer Learning with EfficientNetB0 and implementing a strategic Fine-Tuning phase, we created a model that is both accurate and computationally efficient enough to run on a standard laptop.

**Future Improvements:**
1.  **Explainability**: Implementing Grad-CAM heatmaps to show *where* the image is fake.
2.  **Video Support**: Extending the model to analyze video frames for temporal consistency.
3.  **Larger Models**: Testing on EfficientNetB3 or B7 types for potentially higher accuracy (at the cost of speed).

---
 
